# ğŸš€ Data Engineering End-to-End Pipeline

### ğŸ¯ **Goal**
This project demonstrates a complete **real-time data engineering pipeline** that simulates financial transactions flowing through a modern data architecture.  
Data flows through **Kafka â†’ MinIO (Raw Zone) â†’ Spark (ETL) â†’ MinIO (Processed Zone) â†’ Postgres â†’ Metabase** for analytics and visualization.

---

## ğŸ§± **Architecture Overview**

```
         â”Œâ”€â”€â”€â”€â”€â”€â”€â”
         â”‚  Kafka   â”‚  â†â”€ Transaction Producer (Python)
         â””â”€â”€â”Œâ”€â”€â”€â”€â”
              â”‚
              â–¼
         â”Œâ”€â”€â”€â”€â”€â”€â”€â”
         â”‚  MinIO   â”‚  â†â”€ Raw Zone Storage (JSON)
         â””â”€â”€â”Œâ”€â”€â”€â”€â”
              â”‚
              â–¼
         â”Œâ”€â”€â”€â”€â”€â”€â”€â”
         â”‚  Spark   â”‚  â†â”€ Cleans & Transforms data â†’ Writes to Processed Zone (Parquet)
         â””â”€â”€â”Œâ”€â”€â”€â”€â”
              â”‚
              â–¼
         â”Œâ”€â”€â”€â”€â”€â”€â”€â”
         â”‚ Postgres â”‚  â†â”€ Stores curated data for BI consumption
         â””â”€â”€â”Œâ”€â”€â”€â”€â”
              â”‚
              â–¼
         â”Œâ”€â”€â”€â”€â”€â”€â”€â”
         â”‚ Metabase â”‚  â†â”€ Visualizes transactions and insights
         â””â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ§© **Technologies Used**

| Component | Purpose | Technology |
|------------|----------|-------------|
| **Data Ingestion** | Stream real-time transactions | ğŸ”¸ Apache Kafka |
| **Object Storage** | Store raw & processed files | ğŸŸ¢ MinIO (S3-compatible) |
| **ETL Processing** | Clean, transform, and load data | ğŸ”¥ Apache Spark |
| **Database** | Store curated data | ğŸ˜ PostgreSQL |
| **Visualization** | Build dashboards | ğŸ“Š Metabase |
| **Containerization** | Run all services together | ğŸ³ Docker & Docker Compose |

---

## âš™ï¸ **Setup & Run**

### 1ï¸âƒ£ Start All Services
```bash
docker-compose up -d
```

âœ… This spins up:
- Zookeeper  
- Kafka  
- Spark Master & Worker  
- MinIO  
- PostgreSQL  
- Metabase  

Check:
```bash
docker ps
```

---

### 2ï¸âƒ£ Produce Transactions to Kafka
Run the Python producer to send fake transactions:
```bash
python scripts/producer_to_kafka.py
```

---

### 3ï¸âƒ£ Consume from Kafka â†’ MinIO (Raw Zone)
This stores each Kafka message as a `.json` file in MinIO:
```bash
python scripts/consumer_to_minio.py
```

Check your **MinIO console** at:  
ğŸ”— [http://localhost:9000](http://localhost:9000)  
(username: `minioadmin`, password: `minioadmin`)

---

### 4ï¸âƒ£ Run ETL in Spark
Now process data from MinIO raw â†’ curated:
```bash
docker exec -it spark-master bash -lc '/opt/spark/bin/spark-submit \
  --packages org.apache.hadoop:hadoop-aws:3.3.4,com.amazonaws:aws-java-sdk-bundle:1.12.262 \
  --conf spark.hadoop.fs.s3a.endpoint=http://minio:9000 \
  --conf spark.hadoop.fs.s3a.path.style.access=true \
  --conf spark.hadoop.fs.s3a.access.key=minioadmin \
  --conf spark.hadoop.fs.s3a.secret.key=minioadmin \
  --conf spark.hadoop.fs.s3a.connection.ssl.enabled=false \
  /opt/spark-app/etl_spark_to_processed.py --endpoint http://minio:9000'
```

This creates **clean Parquet files** in the `processed` bucket and writes curated data into **PostgreSQL**.

---

### 5ï¸âƒ£ Visualize in Metabase
- Open: [http://localhost:3000](http://localhost:3000)  
- Connect the `analytics` database (Postgres)  
- Explore tables and create dashboards.

---

## ğŸ“‚ **Project Structure**

```
data-engineering-project/
â”‚
â”œâ”€â”€ docker/
â”‚   â”œâ”€â”€ docker-compose.yml
â”‚   â””â”€â”€ configs/
â”‚
â”œâ”€â”€ scripts/
â”‚   â”œâ”€â”€ producer_to_kafka.py
â”‚   â”œâ”€â”€ consumer_to_minio.py
â”‚   â””â”€â”€ etl_spark_to_processed.py
â”‚
â”œâ”€â”€ spark-app/
â”‚   â””â”€â”€ etl_spark_to_processed.py
â”‚
â””â”€â”€ README.md
```

---

## ğŸ§  **Key Learnings**
- End-to-end orchestration of **data streaming â†’ storage â†’ ETL â†’ analytics**.
- Real-time ingestion with **Kafka** and object-based persistence using **MinIO**.
- Distributed ETL with **PySpark**.
- Post-ETL analytics integration via **PostgreSQL + Metabase**.
- Full containerized environment using **Docker Compose**.

---

## ğŸ§© **Future Enhancements**
- Integrate **Airflow** for scheduling ETL runs.  
- Add **data quality validation** with Great Expectations.  
- Deploy on **AWS (MSK + S3 + EMR + RDS)** for production-scale testing.

---

## ğŸ› ï¸ **Tech Stack Badges**

![Docker](https://img.shields.io/badge/Docker-2496ED?style=for-the-badge&logo=docker&logoColor=white)
![Apache Kafka](https://img.shields.io/badge/Kafka-231F20?style=for-the-badge&logo=apache-kafka&logoColor=white)
![Apache Spark](https://img.shields.io/badge/Spark-E25A1C?style=for-the-badge&logo=apache-spark&logoColor=white)
![PostgreSQL](https://img.shields.io/badge/PostgreSQL-316192?style=for-the-badge&logo=postgresql&logoColor=white)
![MinIO](https://img.shields.io/badge/MinIO-C72E49?style=for-the-badge&logo=minio&logoColor=white)
![Metabase](https://img.shields.io/badge/Metabase-509EE3?style=for-the-badge&logo=metabase&logoColor=white)
![Python](https://img.shields.io/badge/Python-3776AB?style=for-the-badge&logo=python&logoColor=white)

